{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model_path = 'last_288.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Instantiate the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "coordinate = 0.8\n",
    "\n",
    "def return_class_name(class_name):\n",
    "    if class_name == '0':\n",
    "        class_name = 'star'\n",
    "    elif class_name == '3':\n",
    "        class_name = 'water drop'\n",
    "    elif class_name == '4':\n",
    "        class_name = 'man head'\n",
    "    elif class_name == '5':\n",
    "        class_name = 'man picture'\n",
    "    elif class_name == '9':\n",
    "        class_name = 'horse'\n",
    "    elif class_name == '28':\n",
    "        class_name = 'triangle'\n",
    "    elif class_name == '2':\n",
    "        class_name = 'cloud'\n",
    "    elif class_name == '19':\n",
    "        class_name = 'house'\n",
    "    elif class_name == '27':\n",
    "        class_name = 'circle'\n",
    "    elif class_name == '25':\n",
    "        class_name = 'crown'\n",
    "    return class_name\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_boxes(image, boxes, names, conf):\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_id = int(conf[i])\n",
    "        class_name = names[class_id]\n",
    "        class_name = return_class_name(class_name)\n",
    "\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Draw rectangle\n",
    "        cv2.putText(image, class_name, (x1 + 5, y1 + 20)  # - 10)\n",
    "                    , cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)  # Draw text\n",
    "        break\n",
    "\n",
    "def find_most_similar_image(new_image_path, load_features_path,result):\n",
    "\n",
    "    global base_model,coordinate,model\n",
    "\n",
    "    # 1. Predict the category of the new image and extract its features.\n",
    "    # For this example, I'll assume you want to use the bounding box area as the feature.\n",
    "    print('new_image_path = ',new_image_path)\n",
    "\n",
    "    for ret in model.predict(new_image_path):\n",
    "        boxes = ret.boxes\n",
    "        names = ret.names\n",
    "        break\n",
    "\n",
    "    cls = boxes.cls.cpu().numpy()  # class IDs\n",
    "    confidences = boxes.conf.cpu().numpy()  # confidence scores\n",
    "\n",
    "    # Get bounding box coordinates\n",
    "    box_coordinates = boxes.xyxy.cpu().numpy()\n",
    "    encoded_path = cv2.imdecode(np.fromfile(new_image_path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    most_similar_image_path = ''\n",
    "    similarity_percentage = 0.0\n",
    "\n",
    "    if len(range(box_coordinates.shape[0])) == 0:\n",
    "        return most_similar_image_path, similarity_percentage\n",
    "\n",
    "\n",
    "    for i in range(box_coordinates.shape[0]):\n",
    "        x1, y1, x2, y2 = box_coordinates[i]\n",
    "        cropped_img = encoded_path[int(y1):int(y2), int(x1):int(x2)]\n",
    "        break\n",
    "\n",
    "    # Preprocess the cropped image for VGG16\n",
    "    img_data = cv2.resize(cropped_img, (128, 128))\n",
    "    #img_data = keras_image.img_to_array(cropped_img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    # Extract features using VGG16\n",
    "    vgg16_feature = base_model.predict(img_data)\n",
    "    feature_vector_predicted = vgg16_feature.flatten()\n",
    "\n",
    "\n",
    "    class_name = return_class_name(str(int(cls[0])))\n",
    "\n",
    "    # 2. Load stored features\n",
    "    similarity_dic = {}\n",
    "    with open(load_features_path, 'rb') as f:\n",
    "        loaded_features = pickle.load(f)\n",
    "\n",
    "    for key, item in loaded_features.items():\n",
    "        #print(f\"Shape of feature_vector_predicted: {feature_vector_predicted.shape}\")\n",
    "        #print(f\"Shape of item: {item.shape}\")\n",
    "\n",
    "        similarities = cosine_similarity(feature_vector_predicted.reshape(1, -1),\n",
    "                                         item.reshape(1, -1))\n",
    "        similarity_dic[key] = similarities\n",
    "\n",
    "    # Sorting the dictionary by values (high to low)\n",
    "    sorted_items = sorted(similarity_dic.items(), key=lambda x: x[1][0][0], reverse=True)\n",
    "    #print('sorted item = ',sorted_items)\n",
    "\n",
    "\n",
    "    # 4. Select the most similar image\n",
    "    #most_similar_idx = np.argmax(similarities)  # Use argmax because a higher cosine similarity is better.\n",
    "    #most_similar_image_path = stored_images[most_similar_idx]\n",
    "\n",
    "\n",
    "    for sorted_item_image,sorted_item_value in sorted_items:\n",
    "        most_similar_image_path = sorted_item_image\n",
    "        similarity_percentage = sorted_item_value[0][0]\n",
    "        #print(most_similar_image_path,similarity_percentage)\n",
    "        break\n",
    "\n",
    "    #similarity_percentage = similarities[most_similar_idx] * 100  # Convert cosine similarity to percentage\n",
    "\n",
    "    # 5. Display the similarity on the most similar image\n",
    "    img = cv2.imread(os.path.join('sample1000', most_similar_image_path))\n",
    "\n",
    "\n",
    "    #draw box\n",
    "    encoded_path = cv2.imdecode(np.fromfile(os.path.join('sample1000', most_similar_image_path), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    for ret in model.predict(encoded_path):\n",
    "        boxes = ret.boxes\n",
    "        names = ret.names\n",
    "        break\n",
    "\n",
    "\n",
    "    original_image = encoded_path.copy()\n",
    "    box_coordinates = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "    for i in range(box_coordinates.shape[0]):\n",
    "        x1, y1, x2, y2 = box_coordinates[i]\n",
    "        cropped_img = original_image[int(y1):int(y2), int(x1):int(x2)]\n",
    "        break\n",
    "\n",
    "    draw_boxes(encoded_path, boxes.xyxy.cpu().numpy(), names, cls)\n",
    "    most_similar_image_path = os.path.join('result', most_similar_image_path)\n",
    "    Image.fromarray(cv2.cvtColor(encoded_path, cv2.COLOR_BGR2RGB)).save(most_similar_image_path)\n",
    "    #cv2.imshow('Most Similar Image', img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    return most_similar_image_path,similarity_percentage,cropped_img\n",
    "\n",
    "#most_similar_image_path #가장유사한 이미지경로\n",
    "#similarity_percentage #유사도 0~1 \n",
    "#cropped_img #yolo모델로 예측된 box부분의 crop 이미지배열정보\n",
    "\n",
    "new_image_path = 'test sample/circle.jpg'\n",
    "load_features_path = 'features.pkl'\n",
    "result = 'result'\n",
    "find_most_similar_image(new_image_path,load_features_path,result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
